import requests
from bs4 import BeautifulSoup
from datetime import datetime
import json

# CategorÃ­as de Wikipedia que vamos a scrapeaer
WIKI_CATEGORIES = [
    # ActuaciÃ³n
    "https://es.wikipedia.org/wiki/CategorÃ­a:Actores_de_teatro_de_MÃ©xico",
    "https://es.wikipedia.org/wiki/CategorÃ­a:Actrices_de_teatro_de_MÃ©xico",
    "https://es.wikipedia.org/wiki/CategorÃ­a:Actores_de_MÃ©xico",
    "https://es.wikipedia.org/wiki/CategorÃ­a:Actrices_de_MÃ©xico",

    # DirecciÃ³n y dramaturgia
    "https://es.wikipedia.org/wiki/CategorÃ­a:Directores_de_teatro_de_MÃ©xico",
    "https://es.wikipedia.org/wiki/CategorÃ­a:Dramaturgos_de_MÃ©xico",
    "https://es.wikipedia.org/wiki/CategorÃ­a:Dramaturgas_de_MÃ©xico",

    # EscenografÃ­a y producciÃ³n
    "https://es.wikipedia.org/wiki/CategorÃ­a:EscenÃ³grafos_de_MÃ©xico",
    "https://es.wikipedia.org/wiki/CategorÃ­a:Productores_de_teatro_de_MÃ©xico",

    # GenÃ©ricas
    "https://es.wikipedia.org/wiki/CategorÃ­a:Teatro_de_MÃ©xico",
    "https://es.wikipedia.org/wiki/CategorÃ­a:Personas_del_teatro_en_MÃ©xico",
]

def capitalize_name(name):
    return ' '.join(w.capitalize() for w in name.strip().split())

def guess_gender(name):
    # Listas de nombres comunes
    common_female = {
        'marÃ­a', 'guadalupe', 'fernanda', 'ana', 'rosa', 'teresa', 'isabel',
        'josefina', 'carmen', 'beatriz', 'claudia', 'luisa', 'patricia',
        'salma', 'verÃ³nica', 'irene', 'martha', 'alondra', 'diana', 'natalia',
        'xÃ³chitl', 'leticia', 'susana', 'marcela'
    }

    common_male = {
        'josÃ©', 'juan', 'luis', 'carlos', 'manuel', 'antonio', 'pedro',
        'miguel', 'jesÃºs', 'jorge', 'fernando', 'andrÃ©s', 'roberto',
        'felipe', 'rafael', 'vicente', 'emilio', 'daniel', 'oscar', 'jaime'
    }

    name_parts = name.strip().lower().split()
    if not name_parts:
        return 'O'

    # Usamos el primer nombre (antes de cualquier apellido)
    first = name_parts[0]

    if first in common_female:
        return 'F'
    if first in common_male:
        return 'M'

    # HeurÃ­stica de terminaciÃ³n sobre el primer nombre
    if first.endswith('a'):
        return 'F'
    if first.endswith('o') or first.endswith('el') or first.endswith('e'):
        return 'M'

    return 'O'

def now():
    return datetime.now().strftime('%Y-%m-%d %H:%M:%S')

def scrape_wikipedia_category(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
    except Exception as e:
        print(f"âŒ Error al acceder a {url}: {e}")
        return []

    soup = BeautifulSoup(response.text, 'html.parser')
    people = []

    for li in soup.select('#mw-pages li'):
        name = li.get_text(strip=True)
        if name:
            people.append({
                'artistic_name': capitalize_name(name),
                'gender': guess_gender(name),
                'subscription_active': '2025-12-31',
                'created_at': now(),
                'updated_at': now()
            })

    return people

def get_existing_people():
    with open('.jwt') as f:
        token = ''.join(f.read().split())  # limpiar caracteres invisibles

    headers = {
        'Authorization': f'Bearer {token}'
    }

    url = 'https://back.todoteatro.mx/admin/people'
    all_people = []
    page = 1

    while True:
        response = requests.get(f'{url}?page={page}', headers=headers)
        if response.status_code != 200:
            print(f'âŒ Error al obtener pÃ¡gina {page}: {response.status_code}')
            break

        data = response.json()
        all_people.extend(data['data'])

        if not data['next_page_url']:
            break
        page += 1

    return set(p['artistic_name'].strip().lower() for p in all_people if p.get('artistic_name'))

# ğŸ”¹ Inicia el proceso
scraped = []
for url in WIKI_CATEGORIES:
    print(f"ğŸ” Scrapeando: {url}")
    scraped.extend(scrape_wikipedia_category(url))

# ğŸ”„ Eliminar duplicados entre categorÃ­as
seen = set()
filtered_scraped = []
for person in scraped:
    key = person['artistic_name'].strip().lower()
    if key not in seen:
        filtered_scraped.append(person)
        seen.add(key)
scraped = filtered_scraped

# ğŸ” Obtener existentes en backend
existing = get_existing_people()

# ğŸ§¾ Filtrar nuevos
to_insert = [p for p in scraped if p['artistic_name'].strip().lower() not in existing]

# ğŸ“Š Mostrar resultados
print(f"\nğŸ” Personas encontradas en Wikipedia: {len(scraped)}")
print(f"ğŸ§¾ Ya existen en la BD: {len(scraped) - len(to_insert)}")
print(f"ğŸ†• Se insertarÃ­an: {len(to_insert)}\n")

print(json.dumps(to_insert, indent=2, ensure_ascii=False))
